{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# pip install beautifulsoup4\n",
    "# pip install wordcloud import WordCloud\n",
    "# pip install jieba\n",
    "from bs4 import BeautifulSoup\n",
    "'''\n",
    "BeautifulSoup是Python里最受欢迎的HTML解析库之一。\n",
    "它可以提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。\n",
    "它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。\n",
    "# find_all( name , attrs , recursive , text , **kwargs,limit  );\n",
    "# findAll返回值是个列表\n",
    "# name:tag的name,可传字符串、正则表达式、列表、True、方法\n",
    "# attrs:属性\n",
    "# recursive:只想搜索tag的直接子节点,可以使用参数 recursive=False，否则搜索所有子孙节点\n",
    "# text:搜文档中的字符串内容,可传字符串、正则表达式、列表、True\n",
    "# **kwargs:搜文档中的字符串内容,可传字符串、正则表达式、列表、True\n",
    "find(tag, attributes, recursive, text, **kwargs) 输出第一个可匹配对象\n",
    "# find(name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
    "# find返回的是字符串值，而且是返回从头查找到的第一个tag对\n",
    "\n",
    "soup.findAll(['title', 'p'])\n",
    "soup.findAll(lambda tag: len(tag.attrs) == 2)   查找两个并仅有两个属性的标签\n",
    "soup.findAll(lambda tag: len(tag.name) == 1and not tag.attrs)  寻找单个字符为标签名并且没有属性的标签\n",
    "soup.findAll(align=\"center\") 查找拥有属性align且值为center的 所有标签\n",
    "soup.find(\"b\", { \"class\" : \"lime\" })\n",
    "import re\n",
    "soup.findAll(re.compile('^b'))\n",
    "soup.findAll(id=re.compile(\"para$\"))\n",
    "'''\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "def run_demo():\n",
    "    soup = BeautifulSoup(html_doc,'html.parser')\n",
    "    # 第一个参数是要匹配的内容\n",
    "    # 第二个参数是beautifulsoup要采用的模块,即规则\n",
    "    # html.parser是python内置的结构匹配方法，但是效率不如lxml所以不常用\n",
    "    # lxml 采用lxml模块\n",
    "    # html5lib,该模块可以将内容转换成html5对象\n",
    "    # 若想要以上功能,就需要具备对应的模块，比如使用lxml就要安装lxml\n",
    "    print ('获取所有的链接：')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        print (link.name,link['href'],link.get_text())\n",
    "\n",
    "    print ('获取lacie的链接')\n",
    "    link_node = soup.find('a',href='http://example.com/lacie')\n",
    "    print (link_node.name, link_node['href'], link_node.get_text())\n",
    "    print ('获取正则匹配')\n",
    "    link_node = soup.find('a', href=re.compile(r'ill'))\n",
    "    print (link_node.name, link_node['href'], link_node.get_text())\n",
    "    #a http://example.com/tillie Tillie\n",
    "\n",
    "    print ('获取p段落文字')\n",
    "    p_node = soup.find('p', class_='title')\n",
    "    print(p_node)\n",
    "    print (p_node.name,p_node.get_text())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "# beautifulsoup\n",
    "\n",
    "# 导入beautifulsoup模块\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML例子\n",
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>\n",
    "            index\n",
    "        </title>\n",
    "    </head>\n",
    "    <body>\n",
    "         <div>\n",
    "                <ul>\n",
    "                     <li id=\"flask\"class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n",
    "                    <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n",
    "                    <li cvlass=\"item-inactie\"><a href=\"link3.html\">third item</a></li>\n",
    "                    <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n",
    "                    <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n",
    "                 </ul>\n",
    "         </div>\n",
    "        <li> hello world </li>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# 构建beautifulsoup实例\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "# find查找一次\n",
    "li = soup.find('li')\n",
    "print('find_li:',li)\n",
    "print('li.text(返回标签的内容):',li.text)\n",
    "print('li.attrs(返回标签的属性):',li.attrs)\n",
    "print('li.string(返回标签内容为字符串):',li.string)\n",
    "print(50*'*','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find可以通过'属性 = 值'的方法进行select\n",
    "li = soup.find(id = 'flask')\n",
    "print(li,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为class是python的保留关键字，所以无法直接查找class这个关键字\n",
    "# 有两种方法可以进行class属性查询\n",
    "# 第一种:在attrs属性用字典进行传递参数\n",
    "find_class = soup.find(attrs={'class':'item-1'})\n",
    "print('findclass:',find_class,'\\n')\n",
    "# 第二种:BeautifulSoup中的特别关键字参数class_\n",
    "beautifulsoup_class_ = soup.find(class_ = 'item-1')\n",
    "print('BeautifulSoup_class_:',beautifulsoup_class_,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all 查找所有\n",
    "li_all = soup.find_all('li')\n",
    "for li in li_all:\n",
    "    print('---')\n",
    "    print('匹配到的li:',li)\n",
    "    print('li的内容:',li.text)\n",
    "    print('li的属性:',li.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最灵活的使用方式\n",
    "li_quick = soup.find_all(attrs={'class':'item-1'})\n",
    "for li_quick in li_quick:\n",
    "    print('最灵活的查找方法:',li_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''苏宁网站案例'''\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=r'http://www.suning.com/'\n",
    "res = urllib.request.urlopen(url)\n",
    "htmlfile = res.read()\n",
    "mysoup = BeautifulSoup(htmlfile, 'lxml')\n",
    "# print(mysoup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all(content=\"app-id=537508092\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all('meta',attrs = {\"content\":\"app-id=537508092\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all('meta',content=\"app-id=537508092\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all('meta')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mysoup.head.find_all('meta')),mysoup.head.find_all('meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mysoup.head.find_all('meta',recursive=False)),mysoup.head.find_all('meta',recursive=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all(text=re.compile(\".*价格.*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find_all('meta',limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.find(content=\"app-id=537508092\"))\n",
    "print(mysoup.head.find('meta',attrs = {\"content\":\"app-id=537508092\"}))\n",
    "print(mysoup.head.find(text=re.compile(\".*价格.*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_parents()返回所有祖先节点的列表，find_parent()返回直接父节点\n",
    "print(mysoup.title.find_parent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.find_parent().find_all('link')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.find_parents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_next_siblings()返回后面所有兄弟节点的列表，find_next_sibling()返回后面第一个兄弟节点\n",
    "print(mysoup.title.find_next_sibling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.find_next_siblings())\n",
    "# find_previous_siblings()返回后面所有兄弟节点的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_previous_sibling()返回前面第一个兄弟节点\n",
    "print(mysoup.title.find_previous_sibling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.find_previous_siblings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all_next()返回节点后所有符合条件的节点的列表, find_next()返回节点后第一个符合条件的节点\n",
    "print(mysoup.title.find_next('link'))\n",
    "print(mysoup.title.find_all_next('link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all_previous()返回节点前所有符合条件的节点, find_previous()返回节点前第一个符合条件的节点\n",
    "print(mysoup.title.find_previous('link'))\n",
    "print(mysoup.title.find_all_previous('link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS选择器，用soup.select()方法，返回类型是list\n",
    "# 通过标签名查找，不加任何修饰；通过类名查找,类名前加'.'；通过id名查找,id名前加'#'；属性用[]括起来\n",
    "print(mysoup.select('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.head.select('meta[name=\"pageid\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.body.select('a[target=\"_blank\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.body.select('#_TOP_BANNER_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.select('em'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在li标签中查找，class=\"title\"的p标签\n",
    "print(mysoup.select('li p[class=\"title\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在div标签中，查找class等于login的标签，标签名与class中间用空格分开\n",
    "print(mysoup.select('div .login'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在a标签中，查找id等于waitPayCounts的标签，标签名与id中间用空格分开\n",
    "print(mysoup.select('a #waitPayCounts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在id等于J_total_num_box的标签中，查找id等于showTotalQty的标签\n",
    "print(mysoup.select('#J_total_num_box #showTotalQty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在class等于ng-vip-union的标签中，查找class等于ng-iconfont的标签\n",
    "print(mysoup.select('.ng-vip-union .ng-iconfont'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在class等于J_total_num_box的标签中，查找id等于showTotalQty的标签\n",
    "print(mysoup.select('.total-num-box #showTotalQty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree  #导入lxml库的etree模块\n",
    "headers={\n",
    "        'user-agent': ***********\n",
    "    }\n",
    "url = \"https://movie.douban.com/subject/33404425/comments\"\n",
    "r = requests.get(url,headers=headers).text\n",
    "s = etree.HTML(r)  #构造一个XPath解析对象并对HTML文本进行自动修正\n",
    "file = s.xpath('//*[@id=\"comments\"]/div[1]/div[2]/p/span/text()')\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree  #导入lxml库的etree模块\n",
    "headers={\n",
    "        'user-agent': '****************'\n",
    "    }\n",
    "url = \"https://movie.douban.com/subject/33404425/comments\"\n",
    "r = requests.get(url,headers=headers).text\n",
    "s = etree.HTML(r)  #构造一个XPath解析对象并对HTML文本进行自动修正\n",
    "file = s.xpath('//*[@id=\"comments\"]/div/div[2]/p/span/text()')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "for i in file:\n",
    "    print(count,i)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree  #导入lxml库的etree模块\n",
    "headers={\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'\n",
    "    }\n",
    "url = \"https://movie.douban.com/subject/33404425/comments\"\n",
    "r = requests.get(url,headers=headers).text\n",
    "s = etree.HTML(r)  #构造一个XPath解析对象并对HTML文本进行自动修正\n",
    "comment={}\n",
    "comment['评论人']=s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/a/text()\")\n",
    "comment['评论'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/p/span/text()\")\n",
    "comment['评分']=[elem[7] for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[2]/@class\")]\n",
    "comment['时间']=[elem.strip() for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[3]/text()\")]\n",
    "comment['有用'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[1]/span/text()\")\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree  #导入lxml库的etree模块\n",
    "headers={\n",
    "        'user-agent': '********************'\n",
    "    }\n",
    "url = \"https://movie.douban.com/subject/33404425/comments\"\n",
    "r = requests.get(url,headers=headers).text\n",
    "s = etree.HTML(r)  #构造一个XPath解析对象并对HTML文本进行自动修正\n",
    "comment={}\n",
    "comment['评论人']=s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/a/text()\")\n",
    "comment['评论'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/p/span/text()\")\n",
    "try:\n",
    "    comment['评分']=[elem[7] for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[2]/@class\")]\n",
    "except:\n",
    "    comment['评分'] = None\n",
    "comment['时间']=[elem.strip() for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[3]/text()\")]\n",
    "comment['有用'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[1]/span/text()\")\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree  #导入lxml库的etree模块\n",
    "import pandas as pd\n",
    "headers={\n",
    "        'user-agent': '******************************'\n",
    "    }\n",
    "result = pd.DataFrame()\n",
    "url = \"https://movie.douban.com/subject/33404425/comments\"\n",
    "r = requests.get(url,headers=headers).text\n",
    "s = etree.HTML(r)  #构造一个XPath解析对象并对HTML文本进行自动修正\n",
    "comment={}\n",
    "comment['评论人']=s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/a/text()\")\n",
    "comment['评论'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/p/span/text()\")\n",
    "try:\n",
    "    comment['评分']=[elem[7] for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[2]/@class\")]\n",
    "except:\n",
    "    comment['评分'] = None\n",
    "comment['时间']=[elem.strip() for elem in s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[2]/span[3]/text()\")]\n",
    "comment['有用'] = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/h3/span[1]/span/text()\")\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(comment)\n",
    "#把结果一步存为EXCEL\n",
    "result.to_csv('D:/豆瓣短评信息.csv',encoding=\"utf_8_sig\",index=False)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改进后的数据爬虫代码\n",
    "import pandas as pd\n",
    "import time\n",
    "headers={\n",
    "        'user-agent': '************************'\n",
    "    }\n",
    "result = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    try:\n",
    "        base_url='https://movie.douban.com/subject/33404425/comments?start='\n",
    "        url = base_url+str(i*20)+'&limit=20&sort=new_score&status=P'\n",
    "        r = requests.get(url, timeout=30, headers=headers).text\n",
    "    except:\n",
    "        print('request is error!')\n",
    "    try:\n",
    "        s = etree.HTML(r)\n",
    "        file = s.xpath(\"/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/p/span/text()\")\n",
    "    except ParseError as e:\n",
    "        print(e.position)\n",
    "    cache = pd.DataFrame(file)\n",
    "    result = pd.concat([result,cache])\n",
    "    df = pd.DataFrame(file)\n",
    "    time.sleep(2)\n",
    "    print('我们正在爬取：第{}页评论'.format(i + 1))\n",
    "result.head()\n",
    "#把结果一步存为EXCEL\n",
    "result.to_csv('d:/pinglun.csv',encoding=\"utf_8_sig\",header= False,index=False)\n",
    "print(\"the programming is over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinglun=pd.read_csv('d:/pinglun.csv',header=None)\n",
    "pinglun.columns = [\"评论\"]\n",
    "results=pd.DataFrame(pinglun)\n",
    "results.tail(10)\n",
    "# print(pinglun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###生成txt文件的词云\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import PIL\n",
    "import jieba\n",
    "import os\n",
    "import re\n",
    "import pandas as pd  #查看分词后结果\n",
    "import numpy    #numpy计算包\n",
    "text = open(\"d:/pinglun.csv\",\"rb\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_chinese(txt):\n",
    "    txt=txt.decode('utf-8')\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]\")\n",
    "    return \"\".join(pattern.findall(txt))\n",
    "clean_text=extract_chinese(text)\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#结巴分词\n",
    "jieba.load_userdict(\"d:/yinmidejiaoluo.txt\")\n",
    "wordlist = jieba.cut_for_search(clean_text) #搜索引擎模式识别，本例子适合此模式\n",
    "#wordlist = jieba.cut(text) # 精确识别---新词识别\n",
    "text = \" \".join(wordlist)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open(\"d:/stopwords.txt\",\"rb\").read().decode('UTF-8').strip()\n",
    "# print(stopwords)\n",
    "#把分词后的txt写入文本文件\n",
    "fenciTxt  = open(\"d:/fenci.txt\",\"w+\")\n",
    "fenciTxt.writelines(text)\n",
    "fenciTxt.close()\n",
    "print(\"File has been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#设置词云\n",
    "# back_coloring = imread(path.join(d, \"./image/love.jpg\")) #背景图片\n",
    "wc = WordCloud(background_color = \"white\", #设置背景颜色\n",
    "               #mask = \"图片\",  #设置背景图片\n",
    "               max_words = 50, #设置最大显示的字数\n",
    "               stopwords = stopwords, #设置停用词\n",
    "            #    font_path=font,\n",
    "               font_path='C:\\Windows\\Fonts\\msyh.ttc',\n",
    "        #设置中文字体，使得词云可以显示（词云默认字体是“DroidSansMono.ttf字体库”，不支持中文）\n",
    "               max_font_size = 70,  #设置字体最大值\n",
    "               random_state = 40, #设置有多少种随机生成状态，即有多少种配色方案\n",
    "               width=800,\n",
    "               height=600,\n",
    "    )\n",
    "myword = wc.generate(text)#生成词云\n",
    "myword.to_file(\"d:/隐蔽的角落_cloud.jpg\")\n",
    "#展示词云图·\n",
    "\n",
    "plt.imshow(myword)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
